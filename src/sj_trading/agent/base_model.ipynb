{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe04742b",
   "metadata": {},
   "source": [
    "## Configuration for fine-tuning and vLLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499ee951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "login(token=hf_token) # or login by terminal `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb9b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 17:55:09 [config.py:823] This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 06-17 17:55:09 [config.py:1946] Defaulting to use mp for distributed inference\n",
      "INFO 06-17 17:55:09 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 06-17 17:55:10 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 06-17 17:55:10 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 06-17 17:55:10 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-17 17:55:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_0dcb5c10'), local_subscribe_addr='ipc:///tmp/398a8a39-4a00-488b-9413-4bea80c6b002', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-17 17:55:11 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c9d4fe12310>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ca914f4a'), local_subscribe_addr='ipc:///tmp/66ecbf4c-cc5b-4a19-b0c8-d4276b3e650a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-17 17:55:11 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c9f04fe8fd0>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bbc68552'), local_subscribe_addr='ipc:///tmp/4b26e2ca-9911-433b-ba62-33efef6a6388', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:11 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "INFO 06-17 17:55:11 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:11 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:11 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:11 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ttsai/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 06-17 17:55:11 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ttsai/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m WARNING 06-17 17:55:11 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 06-17 17:55:11 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_c166c3ce'), local_subscribe_addr='ipc:///tmp/c0a1b551-a139-455e-877d-ace1c5c8a821', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:11 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m WARNING 06-17 17:55:11 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:11 [gpu_model_runner.py:1595] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:11 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m WARNING 06-17 17:55:11 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:11 [gpu_model_runner.py:1595] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:11 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:11 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:11 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:11 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:12 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:12 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4193e2b3e6544c8380543810a689f8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:13 [weight_utils.py:308] Time spent downloading weights for mistralai/Mistral-7B-Instruct-v0.3: 0.746532 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:14 [default_loader.py:272] Loading weights took 2.07 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:15 [gpu_model_runner.py:1624] Model loading took 6.7584 GiB and 3.347486 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:16 [default_loader.py:272] Loading weights took 2.26 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:16 [gpu_model_runner.py:1624] Model loading took 6.7584 GiB and 4.563219 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:20 [backends.py:462] Using cache directory: /home/ttsai/.cache/vllm/torch_compile_cache/49088ecba2/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:20 [backends.py:472] Dynamo bytecode transform time: 3.49 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:20 [backends.py:462] Using cache directory: /home/ttsai/.cache/vllm/torch_compile_cache/49088ecba2/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:20 [backends.py:472] Dynamo bytecode transform time: 3.53 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m [rank1]:W0617 17:55:20.751000 282555 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m [rank0]:W0617 17:55:20.827000 282554 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:21 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:21 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:33 [backends.py:173] Compiling a graph for general shape takes 12.60 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:33 [backends.py:173] Compiling a graph for general shape takes 12.54 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:45 [monitor.py:34] torch.compile takes 16.07 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:45 [monitor.py:34] torch.compile takes 16.09 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:55:46 [gpu_worker.py:227] Available KV cache memory: 6.55 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:55:46 [gpu_worker.py:227] Available KV cache memory: 6.55 GiB\n",
      "INFO 06-17 17:55:46 [kv_cache_utils.py:715] GPU KV cache size: 107,360 tokens\n",
      "INFO 06-17 17:55:46 [kv_cache_utils.py:719] Maximum concurrency for 2,048 tokens per request: 52.42x\n",
      "INFO 06-17 17:55:46 [kv_cache_utils.py:715] GPU KV cache size: 107,360 tokens\n",
      "INFO 06-17 17:55:46 [kv_cache_utils.py:719] Maximum concurrency for 2,048 tokens per request: 52.42x\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=282554)\u001b[0;0m INFO 06-17 17:56:06 [gpu_model_runner.py:2048] Graph capturing finished in 20 secs, took 0.71 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=282555)\u001b[0;0m INFO 06-17 17:56:06 [gpu_model_runner.py:2048] Graph capturing finished in 20 secs, took 0.71 GiB\n",
      "INFO 06-17 17:56:06 [core.py:171] init engine (profile, create kv cache, warmup model) took 50.22 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "###-- Critical Environment Memo --###\n",
    "# CUDA: 12.9\n",
    "# Driver version: 575.57.08\n",
    "# torch version: 2.7.1 + cu128\n",
    "###-------------------------------###\n",
    "\n",
    "# Load the model\n",
    "llm = LLM(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    tokenizer_mode=\"mistral\",\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.90,\n",
    "    max_model_len=2048,\n",
    "    max_num_seqs=64,\n",
    "    tensor_parallel_size=2,\n",
    ")\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f475c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa0240a3ed04381800f9d2191a84de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8747b5c8c50f41368819b8d97e661c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "Tesla, Inc. (TSLA) is an American electric vehicle (EV) and clean energy company that has disrupted the traditional automotive industry with its innovative technology and commitment to sustainability. As of 2021, Tesla is the world's most valuable automaker by market capitalization, and its stock has been notorious for its volatility.\n",
      "\n",
      "Financial Analysis:\n",
      "\n",
      "1. Revenue: In 2020, Tesla reported total revenues of $31.5 billion, an increase of 36% compared to 2019. The growth was driven by a 48% increase in vehicle deliveries to 509,738 units, as well as a 45% increase in regulatory credits to $1.2 billion.\n",
      "\n",
      "2. Gross Margin: Tesla's gross margin improved significantly in 2020, reaching 27.3%, compared to 21.7% in 2019. The improvement was due to a higher mix of Model 3 and Model Y sales, which have higher margins, as well as cost-cutting measures and economies of\n"
     ]
    }
   ],
   "source": [
    "# Genearate with the LoRA adapter\n",
    "outputs = llm.generate(\n",
    "    [\"Write a financial analysis of Tesla.\"],\n",
    "    sampling_params\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77658fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up model if necessary\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# If using distributed parallelism, import destroy functions\n",
    "from vllm.distributed.parallel_state import (\n",
    "    destroy_model_parallel, destroy_distributed_environment\n",
    ")\n",
    "\n",
    "# Delete model parallel/distributed environments\n",
    "destroy_model_parallel()\n",
    "destroy_distributed_environment()\n",
    "\n",
    "# Delete the LLM object\n",
    "del llm\n",
    "\n",
    "# Clean up Python and GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
