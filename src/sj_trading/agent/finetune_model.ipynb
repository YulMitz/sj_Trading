{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de68072e",
   "metadata": {},
   "source": [
    "### Convert to Hugging Face to PEFT Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import json\n",
    "from safetensors.torch import save_file, load_file\n",
    "\n",
    "# Load params.json and convert to PEFT format\n",
    "with open('/home/ttsai/mulkooo/sj_Trading/experiment/checkpoints/checkpoint_000300/consolidated/params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "adapter_config = {\n",
    "    \"base_model_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"bias\": \"none\",\n",
    "    \"peft_type\": \"LORA\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"r\": params[\"lora\"].get(\"rank\", 32),\n",
    "    \"lora_alpha\": params[\"lora\"].get(\"alpha\", 64),\n",
    "    \"lora_dropout\": params[\"lora\"].get(\"dropout\", 0.1),\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "}\n",
    "\n",
    "# Save as adapter_config.json\n",
    "with open('adapter_config.json', 'w') as f:\n",
    "    json.dump(adapter_config, f, indent=2)\n",
    "\n",
    "# Copy converted config file to sj_Trading/experiment/checkpoints/checkpoint_0003000/consolidated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e60887",
   "metadata": {},
   "source": [
    "### Try to inference using converted LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f523e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 17:28:43 [config.py:823] This model supports multiple tasks: {'generate', 'reward', 'score', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 06-17 17:28:44 [config.py:1946] Defaulting to use mp for distributed inference\n",
      "INFO 06-17 17:28:44 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 06-17 17:28:44 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 06-17 17:28:44 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 06-17 17:28:44 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-17 17:28:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_a010604e'), local_subscribe_addr='ipc:///tmp/553cad84-6e73-47b4-aa83-b44b613ec8df', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-17 17:28:44 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x710578533e10>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_91df39dc'), local_subscribe_addr='ipc:///tmp/c01fd5ce-0927-46bb-ac31-59380a2d01ae', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-17 17:28:44 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7104298d3d10>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e62d3709'), local_subscribe_addr='ipc:///tmp/7f0e4bc0-b67b-4669-ba7a-5e5264d26d5b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:45 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "INFO 06-17 17:28:45 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:45 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:45 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:45 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ttsai/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 06-17 17:28:45 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ttsai/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m WARNING 06-17 17:28:45 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 06-17 17:28:45 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_c9830124'), local_subscribe_addr='ipc:///tmp/9e9d626e-2554-403b-adc0-745920bf12e3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:45 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 06-17 17:28:45 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m WARNING 06-17 17:28:45 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 06-17 17:28:45 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:45 [gpu_model_runner.py:1595] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:45 [gpu_model_runner.py:1595] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:45 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 06-17 17:28:45 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:46 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-17 17:28:46 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:46 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:46 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaecf36112447409f3c48578f0c0884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:48 [default_loader.py:272] Loading weights took 1.50 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:48 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:48 [gpu_model_runner.py:1624] Model loading took 6.8228 GiB and 2.438754 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:49 [default_loader.py:272] Loading weights took 1.98 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:49 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:49 [gpu_model_runner.py:1624] Model loading took 6.8228 GiB and 3.371904 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:55 [backends.py:462] Using cache directory: /home/ttsai/.cache/vllm/torch_compile_cache/022cfb20bf/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:55 [backends.py:472] Dynamo bytecode transform time: 5.54 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:55 [backends.py:462] Using cache directory: /home/ttsai/.cache/vllm/torch_compile_cache/022cfb20bf/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:55 [backends.py:472] Dynamo bytecode transform time: 5.55 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m [rank1]:W0617 17:28:56.221000 274722 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m [rank0]:W0617 17:28:56.241000 274721 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:28:57 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:28:57 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:29:15 [backends.py:173] Compiling a graph for general shape takes 20.19 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:29:15 [backends.py:173] Compiling a graph for general shape takes 20.18 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:29:39 [monitor.py:34] torch.compile takes 25.74 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:29:39 [monitor.py:34] torch.compile takes 25.73 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:29:40 [gpu_worker.py:227] Available KV cache memory: 6.49 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:29:40 [gpu_worker.py:227] Available KV cache memory: 6.49 GiB\n",
      "INFO 06-17 17:29:41 [kv_cache_utils.py:715] GPU KV cache size: 106,288 tokens\n",
      "INFO 06-17 17:29:41 [kv_cache_utils.py:719] Maximum concurrency for 2,048 tokens per request: 51.90x\n",
      "INFO 06-17 17:29:41 [kv_cache_utils.py:715] GPU KV cache size: 106,288 tokens\n",
      "INFO 06-17 17:29:41 [kv_cache_utils.py:719] Maximum concurrency for 2,048 tokens per request: 51.90x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m INFO 06-17 17:30:11 [gpu_model_runner.py:2048] Graph capturing finished in 31 secs, took 0.94 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m INFO 06-17 17:30:11 [gpu_model_runner.py:2048] Graph capturing finished in 31 secs, took 0.94 GiB\n",
      "INFO 06-17 17:30:11 [core.py:171] init engine (profile, create kv cache, warmup model) took 82.30 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527] WorkerProc hit an exception.\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527] WorkerProc hit an exception.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527] Traceback (most recent call last):\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 522, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 522, in worker_busy_loop\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     output = func(*args, **kwargs)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 293, in execute_model\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return func(*args, **kwargs)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 293, in execute_model\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1187, in execute_model\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self._prepare_inputs(scheduler_output))\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 724, in _prepare_inputs\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.set_active_loras(self.input_batch, num_scheduled_tokens)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1187, in execute_model\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/lora_model_runner_mixin.py\", line 80, in set_active_loras\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self._prepare_inputs(scheduler_output))\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return self._set_active_loras(prompt_lora_mapping, token_lora_mapping,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 724, in _prepare_inputs\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/lora_model_runner_mixin.py\", line 69, in _set_active_loras\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.set_active_loras(self.input_batch, num_scheduled_tokens)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.lora_manager.set_active_adapters(lora_requests, lora_mapping)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 170, in set_active_adapters\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/lora_model_runner_mixin.py\", line 80, in set_active_loras\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     set_active_adapters_worker(requests, mapping, self._apply_adapters,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return self._set_active_loras(prompt_lora_mapping, token_lora_mapping,\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/adapter_commons/utils.py\", line 55, in set_active_adapters_worker\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     apply_adapters_func(requests)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/lora_model_runner_mixin.py\", line 69, in _set_active_loras\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.lora_manager.set_active_adapters(lora_requests, lora_mapping)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 230, in _apply_adapters\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 170, in set_active_adapters\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.add_adapter(lora)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 243, in add_adapter\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     set_active_adapters_worker(requests, mapping, self._apply_adapters,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     lora = self._load_adapter(lora_request)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/adapter_commons/utils.py\", line 55, in set_active_adapters_worker\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     apply_adapters_func(requests)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 144, in _load_adapter\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     raise e\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 230, in _apply_adapters\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 119, in _load_adapter\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.add_adapter(lora)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     lora = self._lora_model_cls.from_local_checkpoint(\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 243, in add_adapter\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     lora = self._load_adapter(lora_request)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/models.py\", line 264, in from_local_checkpoint\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     check_unexpected_modules(f)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 144, in _load_adapter\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/models.py\", line 235, in check_unexpected_modules\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     raise e\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     raise ValueError(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 119, in _load_adapter\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527] ValueError: While loading /home/ttsai/mulkooo/sj_Trading/adapter, expected target modules in ['embed_tokens', 'lm_head', 'v_proj', 'gate_proj', 'o_proj', 'up_proj', 'k_proj', 'down_proj', 'q_proj'] but received ['layers.0.attention.wk', 'layers.0.attention.wk', 'layers.0.attention.wo', 'layers.0.attention.wo', 'layers.0.attention.wq', 'layers.0.attention.wq', 'layers.0.attention.wv', 'layers.0.attention.wv', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w3', 'layers.0.feed_forward.w3', 'layers.1.attention.wk', 'layers.1.attention.wk', 'layers.1.attention.wo', 'layers.1.attention.wo', 'layers.1.attention.wq', 'layers.1.attention.wq', 'layers.1.attention.wv', 'layers.1.attention.wv', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w3', 'layers.1.feed_forward.w3', 'layers.10.attention.wk', 'layers.10.attention.wk', 'layers.10.attention.wo', 'layers.10.attention.wo', 'layers.10.attention.wq', 'layers.10.attention.wq', 'layers.10.attention.wv', 'layers.10.attention.wv', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w3', 'layers.10.feed_forward.w3', 'layers.11.attention.wk', 'layers.11.attention.wk', 'layers.11.attention.wo', 'layers.11.attention.wo', 'layers.11.attention.wq', 'layers.11.attention.wq', 'layers.11.attention.wv', 'layers.11.attention.wv', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w3', 'layers.11.feed_forward.w3', 'layers.12.attention.wk', 'layers.12.attention.wk', 'layers.12.attention.wo', 'layers.12.attention.wo', 'layers.12.attention.wq', 'layers.12.attention.wq', 'layers.12.attention.wv', 'layers.12.attention.wv', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w3', 'layers.12.feed_forward.w3', 'layers.13.attention.wk', 'layers.13.attention.wk', 'layers.13.attention.wo', 'layers.13.attention.wo', 'layers.13.attention.wq', 'layers.13.attention.wq', 'layers.13.attention.wv', 'layers.13.attention.wv', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w3', 'layers.13.feed_forward.w3', 'layers.14.attention.wk', 'layers.14.attention.wk', 'layers.14.attention.wo', 'layers.14.attention.wo', 'layers.14.attention.wq', 'layers.14.attention.wq', 'layers.14.attention.wv', 'layers.14.attention.wv', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w3', 'layers.14.feed_forward.w3', 'layers.15.attention.wk', 'layers.15.attention.wk', 'layers.15.attention.wo', 'layers.15.attention.wo', 'layers.15.attention.wq', 'layers.15.attention.wq', 'layers.15.attention.wv', 'layers.15.attention.wv', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w3', 'layers.15.feed_forward.w3', 'layers.16.attention.wk', 'layers.16.attention.wk', 'layers.16.attention.wo', 'layers.16.attention.wo', 'layers.16.attention.wq', 'layers.16.attention.wq', 'layers.16.attention.wv', 'layers.16.attention.wv', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w3', 'layers.16.feed_forward.w3', 'layers.17.attention.wk', 'layers.17.attention.wk', 'layers.17.attention.wo', 'layers.17.attention.wo', 'layers.17.attention.wq', 'layers.17.attention.wq', 'layers.17.attention.wv', 'layers.17.attention.wv', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w3', 'layers.17.feed_forward.w3', 'layers.18.attention.wk', 'layers.18.attention.wk', 'layers.18.attention.wo', 'layers.18.attention.wo', 'layers.18.attention.wq', 'layers.18.attention.wq', 'layers.18.attention.wv', 'layers.18.attention.wv', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w3', 'layers.18.feed_forward.w3', 'layers.19.attention.wk', 'layers.19.attention.wk', 'layers.19.attention.wo', 'layers.19.attention.wo', 'layers.19.attention.wq', 'layers.19.attention.wq', 'layers.19.attention.wv', 'layers.19.attention.wv', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w3', 'layers.19.feed_forward.w3', 'layers.2.attention.wk', 'layers.2.attention.wk', 'layers.2.attention.wo', 'layers.2.attention.wo', 'layers.2.attention.wq', 'layers.2.attention.wq', 'layers.2.attention.wv', 'layers.2.attention.wv', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w3', 'layers.2.feed_forward.w3', 'layers.20.attention.wk', 'layers.20.attention.wk', 'layers.20.attention.wo', 'layers.20.attention.wo', 'layers.20.attention.wq', 'layers.20.attention.wq', 'layers.20.attention.wv', 'layers.20.attention.wv', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w3', 'layers.20.feed_forward.w3', 'layers.21.attention.wk', 'layers.21.attention.wk', 'layers.21.attention.wo', 'layers.21.attention.wo', 'layers.21.attention.wq', 'layers.21.attention.wq', 'layers.21.attention.wv', 'layers.21.attention.wv', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w3', 'layers.21.feed_forward.w3', 'layers.22.attention.wk', 'layers.22.attention.wk', 'layers.22.attention.wo', 'layers.22.attention.wo', 'layers.22.attention.wq', 'layers.22.attention.wq', 'layers.22.attention.wv', 'layers.22.attention.wv', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w3', 'layers.22.feed_forward.w3', 'layers.23.attention.wk', 'layers.23.attention.wk', 'layers.23.attention.wo', 'layers.23.attention.wo', 'layers.23.attention.wq', 'layers.23.attention.wq', 'layers.23.attention.wv', 'layers.23.attention.wv', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w3', 'layers.23.feed_forward.w3', 'layers.24.attention.wk', 'layers.24.attention.wk', 'layers.24.attention.wo', 'layers.24.attention.wo', 'layers.24.attention.wq', 'layers.24.attention.wq', 'layers.24.attention.wv', 'layers.24.attention.wv', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w3', 'layers.24.feed_forward.w3', 'layers.25.attention.wk', 'layers.25.attention.wk', 'layers.25.attention.wo', 'layers.25.attention.wo', 'layers.25.attention.wq', 'layers.25.attention.wq', 'layers.25.attention.wv', 'layers.25.attention.wv', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w3', 'layers.25.feed_forward.w3', 'layers.26.attention.wk', 'layers.26.attention.wk', 'layers.26.attention.wo', 'layers.26.attention.wo', 'layers.26.attention.wq', 'layers.26.attention.wq', 'layers.26.attention.wv', 'layers.26.attention.wv', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w3', 'layers.26.feed_forward.w3', 'layers.27.attention.wk', 'layers.27.attention.wk', 'layers.27.attention.wo', 'layers.27.attention.wo', 'layers.27.attention.wq', 'layers.27.attention.wq', 'layers.27.attention.wv', 'layers.27.attention.wv', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w3', 'layers.27.feed_forward.w3', 'layers.28.attention.wk', 'layers.28.attention.wk', 'layers.28.attention.wo', 'layers.28.attention.wo', 'layers.28.attention.wq', 'layers.28.attention.wq', 'layers.28.attention.wv', 'layers.28.attention.wv', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w3', 'layers.28.feed_forward.w3', 'layers.29.attention.wk', 'layers.29.attention.wk', 'layers.29.attention.wo', 'layers.29.attention.wo', 'layers.29.attention.wq', 'layers.29.attention.wq', 'layers.29.attention.wv', 'layers.29.attention.wv', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w3', 'layers.29.feed_forward.w3', 'layers.3.attention.wk', 'layers.3.attention.wk', 'layers.3.attention.wo', 'layers.3.attention.wo', 'layers.3.attention.wq', 'layers.3.attention.wq', 'layers.3.attention.wv', 'layers.3.attention.wv', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w3', 'layers.3.feed_forward.w3', 'layers.30.attention.wk', 'layers.30.attention.wk', 'layers.30.attention.wo', 'layers.30.attention.wo', 'layers.30.attention.wq', 'layers.30.attention.wq', 'layers.30.attention.wv', 'layers.30.attention.wv', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w3', 'layers.30.feed_forward.w3', 'layers.31.attention.wk', 'layers.31.attention.wk', 'layers.31.attention.wo', 'layers.31.attention.wo', 'layers.31.attention.wq', 'layers.31.attention.wq', 'layers.31.attention.wv', 'layers.31.attention.wv', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w3', 'layers.31.feed_forward.w3', 'layers.4.attention.wk', 'layers.4.attention.wk', 'layers.4.attention.wo', 'layers.4.attention.wo', 'layers.4.attention.wq', 'layers.4.attention.wq', 'layers.4.attention.wv', 'layers.4.attention.wv', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w3', 'layers.4.feed_forward.w3', 'layers.5.attention.wk', 'layers.5.attention.wk', 'layers.5.attention.wo', 'layers.5.attention.wo', 'layers.5.attention.wq', 'layers.5.attention.wq', 'layers.5.attention.wv', 'layers.5.attention.wv', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w3', 'layers.5.feed_forward.w3', 'layers.6.attention.wk', 'layers.6.attention.wk', 'layers.6.attention.wo', 'layers.6.attention.wo', 'layers.6.attention.wq', 'layers.6.attention.wq', 'layers.6.attention.wv', 'layers.6.attention.wv', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w3', 'layers.6.feed_forward.w3', 'layers.7.attention.wk', 'layers.7.attention.wk', 'layers.7.attention.wo', 'layers.7.attention.wo', 'layers.7.attention.wq', 'layers.7.attention.wq', 'layers.7.attention.wv', 'layers.7.attention.wv', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w3', 'layers.7.feed_forward.w3', 'layers.8.attention.wk', 'layers.8.attention.wk', 'layers.8.attention.wo', 'layers.8.attention.wo', 'layers.8.attention.wq', 'layers.8.attention.wq', 'layers.8.attention.wv', 'layers.8.attention.wv', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w3', 'layers.8.feed_forward.w3', 'layers.9.attention.wk', 'layers.9.attention.wk', 'layers.9.attention.wo', 'layers.9.attention.wo', 'layers.9.attention.wq', 'layers.9.attention.wq', 'layers.9.attention.wv', 'layers.9.attention.wv', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w3', 'layers.9.feed_forward.w3']. Please verify that the loaded LoRA module is correct\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     lora = self._lora_model_cls.from_local_checkpoint(\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527] Traceback (most recent call last):\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 522, in worker_busy_loop\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/models.py\", line 264, in from_local_checkpoint\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     output = func(*args, **kwargs)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     check_unexpected_modules(f)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/models.py\", line 235, in check_unexpected_modules\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     raise ValueError(\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return func(*args, **kwargs)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527] ValueError: While loading /home/ttsai/mulkooo/sj_Trading/adapter, expected target modules in ['embed_tokens', 'lm_head', 'v_proj', 'gate_proj', 'o_proj', 'up_proj', 'k_proj', 'down_proj', 'q_proj'] but received ['layers.0.attention.wk', 'layers.0.attention.wk', 'layers.0.attention.wo', 'layers.0.attention.wo', 'layers.0.attention.wq', 'layers.0.attention.wq', 'layers.0.attention.wv', 'layers.0.attention.wv', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w3', 'layers.0.feed_forward.w3', 'layers.1.attention.wk', 'layers.1.attention.wk', 'layers.1.attention.wo', 'layers.1.attention.wo', 'layers.1.attention.wq', 'layers.1.attention.wq', 'layers.1.attention.wv', 'layers.1.attention.wv', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w3', 'layers.1.feed_forward.w3', 'layers.10.attention.wk', 'layers.10.attention.wk', 'layers.10.attention.wo', 'layers.10.attention.wo', 'layers.10.attention.wq', 'layers.10.attention.wq', 'layers.10.attention.wv', 'layers.10.attention.wv', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w3', 'layers.10.feed_forward.w3', 'layers.11.attention.wk', 'layers.11.attention.wk', 'layers.11.attention.wo', 'layers.11.attention.wo', 'layers.11.attention.wq', 'layers.11.attention.wq', 'layers.11.attention.wv', 'layers.11.attention.wv', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w3', 'layers.11.feed_forward.w3', 'layers.12.attention.wk', 'layers.12.attention.wk', 'layers.12.attention.wo', 'layers.12.attention.wo', 'layers.12.attention.wq', 'layers.12.attention.wq', 'layers.12.attention.wv', 'layers.12.attention.wv', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w3', 'layers.12.feed_forward.w3', 'layers.13.attention.wk', 'layers.13.attention.wk', 'layers.13.attention.wo', 'layers.13.attention.wo', 'layers.13.attention.wq', 'layers.13.attention.wq', 'layers.13.attention.wv', 'layers.13.attention.wv', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w3', 'layers.13.feed_forward.w3', 'layers.14.attention.wk', 'layers.14.attention.wk', 'layers.14.attention.wo', 'layers.14.attention.wo', 'layers.14.attention.wq', 'layers.14.attention.wq', 'layers.14.attention.wv', 'layers.14.attention.wv', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w3', 'layers.14.feed_forward.w3', 'layers.15.attention.wk', 'layers.15.attention.wk', 'layers.15.attention.wo', 'layers.15.attention.wo', 'layers.15.attention.wq', 'layers.15.attention.wq', 'layers.15.attention.wv', 'layers.15.attention.wv', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w3', 'layers.15.feed_forward.w3', 'layers.16.attention.wk', 'layers.16.attention.wk', 'layers.16.attention.wo', 'layers.16.attention.wo', 'layers.16.attention.wq', 'layers.16.attention.wq', 'layers.16.attention.wv', 'layers.16.attention.wv', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w3', 'layers.16.feed_forward.w3', 'layers.17.attention.wk', 'layers.17.attention.wk', 'layers.17.attention.wo', 'layers.17.attention.wo', 'layers.17.attention.wq', 'layers.17.attention.wq', 'layers.17.attention.wv', 'layers.17.attention.wv', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w3', 'layers.17.feed_forward.w3', 'layers.18.attention.wk', 'layers.18.attention.wk', 'layers.18.attention.wo', 'layers.18.attention.wo', 'layers.18.attention.wq', 'layers.18.attention.wq', 'layers.18.attention.wv', 'layers.18.attention.wv', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w3', 'layers.18.feed_forward.w3', 'layers.19.attention.wk', 'layers.19.attention.wk', 'layers.19.attention.wo', 'layers.19.attention.wo', 'layers.19.attention.wq', 'layers.19.attention.wq', 'layers.19.attention.wv', 'layers.19.attention.wv', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w3', 'layers.19.feed_forward.w3', 'layers.2.attention.wk', 'layers.2.attention.wk', 'layers.2.attention.wo', 'layers.2.attention.wo', 'layers.2.attention.wq', 'layers.2.attention.wq', 'layers.2.attention.wv', 'layers.2.attention.wv', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w3', 'layers.2.feed_forward.w3', 'layers.20.attention.wk', 'layers.20.attention.wk', 'layers.20.attention.wo', 'layers.20.attention.wo', 'layers.20.attention.wq', 'layers.20.attention.wq', 'layers.20.attention.wv', 'layers.20.attention.wv', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w3', 'layers.20.feed_forward.w3', 'layers.21.attention.wk', 'layers.21.attention.wk', 'layers.21.attention.wo', 'layers.21.attention.wo', 'layers.21.attention.wq', 'layers.21.attention.wq', 'layers.21.attention.wv', 'layers.21.attention.wv', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w3', 'layers.21.feed_forward.w3', 'layers.22.attention.wk', 'layers.22.attention.wk', 'layers.22.attention.wo', 'layers.22.attention.wo', 'layers.22.attention.wq', 'layers.22.attention.wq', 'layers.22.attention.wv', 'layers.22.attention.wv', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w3', 'layers.22.feed_forward.w3', 'layers.23.attention.wk', 'layers.23.attention.wk', 'layers.23.attention.wo', 'layers.23.attention.wo', 'layers.23.attention.wq', 'layers.23.attention.wq', 'layers.23.attention.wv', 'layers.23.attention.wv', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w3', 'layers.23.feed_forward.w3', 'layers.24.attention.wk', 'layers.24.attention.wk', 'layers.24.attention.wo', 'layers.24.attention.wo', 'layers.24.attention.wq', 'layers.24.attention.wq', 'layers.24.attention.wv', 'layers.24.attention.wv', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w3', 'layers.24.feed_forward.w3', 'layers.25.attention.wk', 'layers.25.attention.wk', 'layers.25.attention.wo', 'layers.25.attention.wo', 'layers.25.attention.wq', 'layers.25.attention.wq', 'layers.25.attention.wv', 'layers.25.attention.wv', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w3', 'layers.25.feed_forward.w3', 'layers.26.attention.wk', 'layers.26.attention.wk', 'layers.26.attention.wo', 'layers.26.attention.wo', 'layers.26.attention.wq', 'layers.26.attention.wq', 'layers.26.attention.wv', 'layers.26.attention.wv', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w3', 'layers.26.feed_forward.w3', 'layers.27.attention.wk', 'layers.27.attention.wk', 'layers.27.attention.wo', 'layers.27.attention.wo', 'layers.27.attention.wq', 'layers.27.attention.wq', 'layers.27.attention.wv', 'layers.27.attention.wv', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w3', 'layers.27.feed_forward.w3', 'layers.28.attention.wk', 'layers.28.attention.wk', 'layers.28.attention.wo', 'layers.28.attention.wo', 'layers.28.attention.wq', 'layers.28.attention.wq', 'layers.28.attention.wv', 'layers.28.attention.wv', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w3', 'layers.28.feed_forward.w3', 'layers.29.attention.wk', 'layers.29.attention.wk', 'layers.29.attention.wo', 'layers.29.attention.wo', 'layers.29.attention.wq', 'layers.29.attention.wq', 'layers.29.attention.wv', 'layers.29.attention.wv', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w3', 'layers.29.feed_forward.w3', 'layers.3.attention.wk', 'layers.3.attention.wk', 'layers.3.attention.wo', 'layers.3.attention.wo', 'layers.3.attention.wq', 'layers.3.attention.wq', 'layers.3.attention.wv', 'layers.3.attention.wv', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w3', 'layers.3.feed_forward.w3', 'layers.30.attention.wk', 'layers.30.attention.wk', 'layers.30.attention.wo', 'layers.30.attention.wo', 'layers.30.attention.wq', 'layers.30.attention.wq', 'layers.30.attention.wv', 'layers.30.attention.wv', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w3', 'layers.30.feed_forward.w3', 'layers.31.attention.wk', 'layers.31.attention.wk', 'layers.31.attention.wo', 'layers.31.attention.wo', 'layers.31.attention.wq', 'layers.31.attention.wq', 'layers.31.attention.wv', 'layers.31.attention.wv', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w3', 'layers.31.feed_forward.w3', 'layers.4.attention.wk', 'layers.4.attention.wk', 'layers.4.attention.wo', 'layers.4.attention.wo', 'layers.4.attention.wq', 'layers.4.attention.wq', 'layers.4.attention.wv', 'layers.4.attention.wv', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w3', 'layers.4.feed_forward.w3', 'layers.5.attention.wk', 'layers.5.attention.wk', 'layers.5.attention.wo', 'layers.5.attention.wo', 'layers.5.attention.wq', 'layers.5.attention.wq', 'layers.5.attention.wv', 'layers.5.attention.wv', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w3', 'layers.5.feed_forward.w3', 'layers.6.attention.wk', 'layers.6.attention.wk', 'layers.6.attention.wo', 'layers.6.attention.wo', 'layers.6.attention.wq', 'layers.6.attention.wq', 'layers.6.attention.wv', 'layers.6.attention.wv', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w3', 'layers.6.feed_forward.w3', 'layers.7.attention.wk', 'layers.7.attention.wk', 'layers.7.attention.wo', 'layers.7.attention.wo', 'layers.7.attention.wq', 'layers.7.attention.wq', 'layers.7.attention.wv', 'layers.7.attention.wv', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w3', 'layers.7.feed_forward.w3', 'layers.8.attention.wk', 'layers.8.attention.wk', 'layers.8.attention.wo', 'layers.8.attention.wo', 'layers.8.attention.wq', 'layers.8.attention.wq', 'layers.8.attention.wv', 'layers.8.attention.wv', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w3', 'layers.8.feed_forward.w3', 'layers.9.attention.wk', 'layers.9.attention.wk', 'layers.9.attention.wo', 'layers.9.attention.wo', 'layers.9.attention.wq', 'layers.9.attention.wq', 'layers.9.attention.wv', 'layers.9.attention.wv', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w3', 'layers.9.feed_forward.w3']. Please verify that the loaded LoRA module is correct\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527] Traceback (most recent call last):\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 293, in execute_model\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 522, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     output = func(*args, **kwargs)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return func(*args, **kwargs)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1187, in execute_model\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py\", line 293, in execute_model\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self._prepare_inputs(scheduler_output))\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     output = self.model_runner.execute_model(scheduler_output,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 724, in _prepare_inputs\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.set_active_loras(self.input_batch, num_scheduled_tokens)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return func(*args, **kwargs)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/lora_model_runner_mixin.py\", line 80, in set_active_loras\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return self._set_active_loras(prompt_lora_mapping, token_lora_mapping,\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1187, in execute_model\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self._prepare_inputs(scheduler_output))\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/lora_model_runner_mixin.py\", line 69, in _set_active_loras\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.lora_manager.set_active_adapters(lora_requests, lora_mapping)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 724, in _prepare_inputs\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 170, in set_active_adapters\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     set_active_adapters_worker(requests, mapping, self._apply_adapters,\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.set_active_loras(self.input_batch, num_scheduled_tokens)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/adapter_commons/utils.py\", line 55, in set_active_adapters_worker\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/lora_model_runner_mixin.py\", line 80, in set_active_loras\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     apply_adapters_func(requests)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     return self._set_active_loras(prompt_lora_mapping, token_lora_mapping,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 230, in _apply_adapters\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.add_adapter(lora)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 243, in add_adapter\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/worker/lora_model_runner_mixin.py\", line 69, in _set_active_loras\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     lora = self._load_adapter(lora_request)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.lora_manager.set_active_adapters(lora_requests, lora_mapping)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 170, in set_active_adapters\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 144, in _load_adapter\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     set_active_adapters_worker(requests, mapping, self._apply_adapters,\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     raise e\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/adapter_commons/utils.py\", line 55, in set_active_adapters_worker\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 119, in _load_adapter\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     apply_adapters_func(requests)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     lora = self._lora_model_cls.from_local_checkpoint(\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 230, in _apply_adapters\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     self.add_adapter(lora)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 243, in add_adapter\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/models.py\", line 264, in from_local_checkpoint\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     lora = self._load_adapter(lora_request)\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     check_unexpected_modules(f)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/models.py\", line 235, in check_unexpected_modules\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 144, in _load_adapter\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     raise ValueError(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     raise e\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/worker_manager.py\", line 119, in _load_adapter\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527] ValueError: While loading /home/ttsai/mulkooo/sj_Trading/adapter, expected target modules in ['embed_tokens', 'lm_head', 'v_proj', 'gate_proj', 'o_proj', 'up_proj', 'k_proj', 'down_proj', 'q_proj'] but received ['layers.0.attention.wk', 'layers.0.attention.wk', 'layers.0.attention.wo', 'layers.0.attention.wo', 'layers.0.attention.wq', 'layers.0.attention.wq', 'layers.0.attention.wv', 'layers.0.attention.wv', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w3', 'layers.0.feed_forward.w3', 'layers.1.attention.wk', 'layers.1.attention.wk', 'layers.1.attention.wo', 'layers.1.attention.wo', 'layers.1.attention.wq', 'layers.1.attention.wq', 'layers.1.attention.wv', 'layers.1.attention.wv', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w3', 'layers.1.feed_forward.w3', 'layers.10.attention.wk', 'layers.10.attention.wk', 'layers.10.attention.wo', 'layers.10.attention.wo', 'layers.10.attention.wq', 'layers.10.attention.wq', 'layers.10.attention.wv', 'layers.10.attention.wv', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w3', 'layers.10.feed_forward.w3', 'layers.11.attention.wk', 'layers.11.attention.wk', 'layers.11.attention.wo', 'layers.11.attention.wo', 'layers.11.attention.wq', 'layers.11.attention.wq', 'layers.11.attention.wv', 'layers.11.attention.wv', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w3', 'layers.11.feed_forward.w3', 'layers.12.attention.wk', 'layers.12.attention.wk', 'layers.12.attention.wo', 'layers.12.attention.wo', 'layers.12.attention.wq', 'layers.12.attention.wq', 'layers.12.attention.wv', 'layers.12.attention.wv', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w3', 'layers.12.feed_forward.w3', 'layers.13.attention.wk', 'layers.13.attention.wk', 'layers.13.attention.wo', 'layers.13.attention.wo', 'layers.13.attention.wq', 'layers.13.attention.wq', 'layers.13.attention.wv', 'layers.13.attention.wv', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w3', 'layers.13.feed_forward.w3', 'layers.14.attention.wk', 'layers.14.attention.wk', 'layers.14.attention.wo', 'layers.14.attention.wo', 'layers.14.attention.wq', 'layers.14.attention.wq', 'layers.14.attention.wv', 'layers.14.attention.wv', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w3', 'layers.14.feed_forward.w3', 'layers.15.attention.wk', 'layers.15.attention.wk', 'layers.15.attention.wo', 'layers.15.attention.wo', 'layers.15.attention.wq', 'layers.15.attention.wq', 'layers.15.attention.wv', 'layers.15.attention.wv', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w3', 'layers.15.feed_forward.w3', 'layers.16.attention.wk', 'layers.16.attention.wk', 'layers.16.attention.wo', 'layers.16.attention.wo', 'layers.16.attention.wq', 'layers.16.attention.wq', 'layers.16.attention.wv', 'layers.16.attention.wv', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w3', 'layers.16.feed_forward.w3', 'layers.17.attention.wk', 'layers.17.attention.wk', 'layers.17.attention.wo', 'layers.17.attention.wo', 'layers.17.attention.wq', 'layers.17.attention.wq', 'layers.17.attention.wv', 'layers.17.attention.wv', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w3', 'layers.17.feed_forward.w3', 'layers.18.attention.wk', 'layers.18.attention.wk', 'layers.18.attention.wo', 'layers.18.attention.wo', 'layers.18.attention.wq', 'layers.18.attention.wq', 'layers.18.attention.wv', 'layers.18.attention.wv', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w3', 'layers.18.feed_forward.w3', 'layers.19.attention.wk', 'layers.19.attention.wk', 'layers.19.attention.wo', 'layers.19.attention.wo', 'layers.19.attention.wq', 'layers.19.attention.wq', 'layers.19.attention.wv', 'layers.19.attention.wv', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w3', 'layers.19.feed_forward.w3', 'layers.2.attention.wk', 'layers.2.attention.wk', 'layers.2.attention.wo', 'layers.2.attention.wo', 'layers.2.attention.wq', 'layers.2.attention.wq', 'layers.2.attention.wv', 'layers.2.attention.wv', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w3', 'layers.2.feed_forward.w3', 'layers.20.attention.wk', 'layers.20.attention.wk', 'layers.20.attention.wo', 'layers.20.attention.wo', 'layers.20.attention.wq', 'layers.20.attention.wq', 'layers.20.attention.wv', 'layers.20.attention.wv', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w3', 'layers.20.feed_forward.w3', 'layers.21.attention.wk', 'layers.21.attention.wk', 'layers.21.attention.wo', 'layers.21.attention.wo', 'layers.21.attention.wq', 'layers.21.attention.wq', 'layers.21.attention.wv', 'layers.21.attention.wv', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w3', 'layers.21.feed_forward.w3', 'layers.22.attention.wk', 'layers.22.attention.wk', 'layers.22.attention.wo', 'layers.22.attention.wo', 'layers.22.attention.wq', 'layers.22.attention.wq', 'layers.22.attention.wv', 'layers.22.attention.wv', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w3', 'layers.22.feed_forward.w3', 'layers.23.attention.wk', 'layers.23.attention.wk', 'layers.23.attention.wo', 'layers.23.attention.wo', 'layers.23.attention.wq', 'layers.23.attention.wq', 'layers.23.attention.wv', 'layers.23.attention.wv', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w3', 'layers.23.feed_forward.w3', 'layers.24.attention.wk', 'layers.24.attention.wk', 'layers.24.attention.wo', 'layers.24.attention.wo', 'layers.24.attention.wq', 'layers.24.attention.wq', 'layers.24.attention.wv', 'layers.24.attention.wv', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w3', 'layers.24.feed_forward.w3', 'layers.25.attention.wk', 'layers.25.attention.wk', 'layers.25.attention.wo', 'layers.25.attention.wo', 'layers.25.attention.wq', 'layers.25.attention.wq', 'layers.25.attention.wv', 'layers.25.attention.wv', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w3', 'layers.25.feed_forward.w3', 'layers.26.attention.wk', 'layers.26.attention.wk', 'layers.26.attention.wo', 'layers.26.attention.wo', 'layers.26.attention.wq', 'layers.26.attention.wq', 'layers.26.attention.wv', 'layers.26.attention.wv', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w3', 'layers.26.feed_forward.w3', 'layers.27.attention.wk', 'layers.27.attention.wk', 'layers.27.attention.wo', 'layers.27.attention.wo', 'layers.27.attention.wq', 'layers.27.attention.wq', 'layers.27.attention.wv', 'layers.27.attention.wv', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w3', 'layers.27.feed_forward.w3', 'layers.28.attention.wk', 'layers.28.attention.wk', 'layers.28.attention.wo', 'layers.28.attention.wo', 'layers.28.attention.wq', 'layers.28.attention.wq', 'layers.28.attention.wv', 'layers.28.attention.wv', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w3', 'layers.28.feed_forward.w3', 'layers.29.attention.wk', 'layers.29.attention.wk', 'layers.29.attention.wo', 'layers.29.attention.wo', 'layers.29.attention.wq', 'layers.29.attention.wq', 'layers.29.attention.wv', 'layers.29.attention.wv', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w3', 'layers.29.feed_forward.w3', 'layers.3.attention.wk', 'layers.3.attention.wk', 'layers.3.attention.wo', 'layers.3.attention.wo', 'layers.3.attention.wq', 'layers.3.attention.wq', 'layers.3.attention.wv', 'layers.3.attention.wv', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w3', 'layers.3.feed_forward.w3', 'layers.30.attention.wk', 'layers.30.attention.wk', 'layers.30.attention.wo', 'layers.30.attention.wo', 'layers.30.attention.wq', 'layers.30.attention.wq', 'layers.30.attention.wv', 'layers.30.attention.wv', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w3', 'layers.30.feed_forward.w3', 'layers.31.attention.wk', 'layers.31.attention.wk', 'layers.31.attention.wo', 'layers.31.attention.wo', 'layers.31.attention.wq', 'layers.31.attention.wq', 'layers.31.attention.wv', 'layers.31.attention.wv', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w3', 'layers.31.feed_forward.w3', 'layers.4.attention.wk', 'layers.4.attention.wk', 'layers.4.attention.wo', 'layers.4.attention.wo', 'layers.4.attention.wq', 'layers.4.attention.wq', 'layers.4.attention.wv', 'layers.4.attention.wv', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w3', 'layers.4.feed_forward.w3', 'layers.5.attention.wk', 'layers.5.attention.wk', 'layers.5.attention.wo', 'layers.5.attention.wo', 'layers.5.attention.wq', 'layers.5.attention.wq', 'layers.5.attention.wv', 'layers.5.attention.wv', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w3', 'layers.5.feed_forward.w3', 'layers.6.attention.wk', 'layers.6.attention.wk', 'layers.6.attention.wo', 'layers.6.attention.wo', 'layers.6.attention.wq', 'layers.6.attention.wq', 'layers.6.attention.wv', 'layers.6.attention.wv', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w3', 'layers.6.feed_forward.w3', 'layers.7.attention.wk', 'layers.7.attention.wk', 'layers.7.attention.wo', 'layers.7.attention.wo', 'layers.7.attention.wq', 'layers.7.attention.wq', 'layers.7.attention.wv', 'layers.7.attention.wv', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w3', 'layers.7.feed_forward.w3', 'layers.8.attention.wk', 'layers.8.attention.wk', 'layers.8.attention.wo', 'layers.8.attention.wo', 'layers.8.attention.wq', 'layers.8.attention.wq', 'layers.8.attention.wv', 'layers.8.attention.wv', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w3', 'layers.8.feed_forward.w3', 'layers.9.attention.wk', 'layers.9.attention.wk', 'layers.9.attention.wo', 'layers.9.attention.wo', 'layers.9.attention.wq', 'layers.9.attention.wq', 'layers.9.attention.wv', 'layers.9.attention.wv', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w3', 'layers.9.feed_forward.w3']. Please verify that the loaded LoRA module is correct\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=274721)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     lora = self._lora_model_cls.from_local_checkpoint(\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527] \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [dump_input.py:69] Dumping input data\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/models.py\", line 264, in from_local_checkpoint\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [dump_input.py:71] V1 LLM engine (v0.9.1) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}, \n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]     check_unexpected_modules(f)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [dump_input.py:79] Dumping scheduler output for model execution:\n",
      "ERROR 06-17 17:30:29 [multiproc_executor.py:527]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/lora/models.py\", line 235, in check_unexpected_modules\n",
      "ERROR 06-17 17:30:29 [dump_input.py:80] SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=0,prompt_token_ids_len=9,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=256, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([1],),num_computed_tokens=0,lora_request=LoRARequest(lora_name='financial_adapter', lora_int_id=1, lora_path='/home/ttsai/mulkooo/sj_Trading/adapter', lora_local_path=None, long_lora_max_len=None, base_model_name=None, tensorizer_config_dict=None))], scheduled_cached_reqs=[], num_scheduled_tokens={0: 9}, total_num_scheduled_tokens=9, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[1], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527]     raise ValueError(\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527] ValueError: While loading /home/ttsai/mulkooo/sj_Trading/adapter, expected target modules in ['embed_tokens', 'lm_head', 'v_proj', 'gate_proj', 'o_proj', 'up_proj', 'k_proj', 'down_proj', 'q_proj'] but received ['layers.0.attention.wk', 'layers.0.attention.wk', 'layers.0.attention.wo', 'layers.0.attention.wo', 'layers.0.attention.wq', 'layers.0.attention.wq', 'layers.0.attention.wv', 'layers.0.attention.wv', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w3', 'layers.0.feed_forward.w3', 'layers.1.attention.wk', 'layers.1.attention.wk', 'layers.1.attention.wo', 'layers.1.attention.wo', 'layers.1.attention.wq', 'layers.1.attention.wq', 'layers.1.attention.wv', 'layers.1.attention.wv', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w3', 'layers.1.feed_forward.w3', 'layers.10.attention.wk', 'layers.10.attention.wk', 'layers.10.attention.wo', 'layers.10.attention.wo', 'layers.10.attention.wq', 'layers.10.attention.wq', 'layers.10.attention.wv', 'layers.10.attention.wv', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w3', 'layers.10.feed_forward.w3', 'layers.11.attention.wk', 'layers.11.attention.wk', 'layers.11.attention.wo', 'layers.11.attention.wo', 'layers.11.attention.wq', 'layers.11.attention.wq', 'layers.11.attention.wv', 'layers.11.attention.wv', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w3', 'layers.11.feed_forward.w3', 'layers.12.attention.wk', 'layers.12.attention.wk', 'layers.12.attention.wo', 'layers.12.attention.wo', 'layers.12.attention.wq', 'layers.12.attention.wq', 'layers.12.attention.wv', 'layers.12.attention.wv', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w3', 'layers.12.feed_forward.w3', 'layers.13.attention.wk', 'layers.13.attention.wk', 'layers.13.attention.wo', 'layers.13.attention.wo', 'layers.13.attention.wq', 'layers.13.attention.wq', 'layers.13.attention.wv', 'layers.13.attention.wv', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w3', 'layers.13.feed_forward.w3', 'layers.14.attention.wk', 'layers.14.attention.wk', 'layers.14.attention.wo', 'layers.14.attention.wo', 'layers.14.attention.wq', 'layers.14.attention.wq', 'layers.14.attention.wv', 'layers.14.attention.wv', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w3', 'layers.14.feed_forward.w3', 'layers.15.attention.wk', 'layers.15.attention.wk', 'layers.15.attention.wo', 'layers.15.attention.wo', 'layers.15.attention.wq', 'layers.15.attention.wq', 'layers.15.attention.wv', 'layers.15.attention.wv', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w3', 'layers.15.feed_forward.w3', 'layers.16.attention.wk', 'layers.16.attention.wk', 'layers.16.attention.wo', 'layers.16.attention.wo', 'layers.16.attention.wq', 'layers.16.attention.wq', 'layers.16.attention.wv', 'layers.16.attention.wv', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w3', 'layers.16.feed_forward.w3', 'layers.17.attention.wk', 'layers.17.attention.wk', 'layers.17.attention.wo', 'layers.17.attention.wo', 'layers.17.attention.wq', 'layers.17.attention.wq', 'layers.17.attention.wv', 'layers.17.attention.wv', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w3', 'layers.17.feed_forward.w3', 'layers.18.attention.wk', 'layers.18.attention.wk', 'layers.18.attention.wo', 'layers.18.attention.wo', 'layers.18.attention.wq', 'layers.18.attention.wq', 'layers.18.attention.wv', 'layers.18.attention.wv', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w3', 'layers.18.feed_forward.w3', 'layers.19.attention.wk', 'layers.19.attention.wk', 'layers.19.attention.wo', 'layers.19.attention.wo', 'layers.19.attention.wq', 'layers.19.attention.wq', 'layers.19.attention.wv', 'layers.19.attention.wv', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w3', 'layers.19.feed_forward.w3', 'layers.2.attention.wk', 'layers.2.attention.wk', 'layers.2.attention.wo', 'layers.2.attention.wo', 'layers.2.attention.wq', 'layers.2.attention.wq', 'layers.2.attention.wv', 'layers.2.attention.wv', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w3', 'layers.2.feed_forward.w3', 'layers.20.attention.wk', 'layers.20.attention.wk', 'layers.20.attention.wo', 'layers.20.attention.wo', 'layers.20.attention.wq', 'layers.20.attention.wq', 'layers.20.attention.wv', 'layers.20.attention.wv', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w3', 'layers.20.feed_forward.w3', 'layers.21.attention.wk', 'layers.21.attention.wk', 'layers.21.attention.wo', 'layers.21.attention.wo', 'layers.21.attention.wq', 'layers.21.attention.wq', 'layers.21.attention.wv', 'layers.21.attention.wv', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w3', 'layers.21.feed_forward.w3', 'layers.22.attention.wk', 'layers.22.attention.wk', 'layers.22.attention.wo', 'layers.22.attention.wo', 'layers.22.attention.wq', 'layers.22.attention.wq', 'layers.22.attention.wv', 'layers.22.attention.wv', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w3', 'layers.22.feed_forward.w3', 'layers.23.attention.wk', 'layers.23.attention.wk', 'layers.23.attention.wo', 'layers.23.attention.wo', 'layers.23.attention.wq', 'layers.23.attention.wq', 'layers.23.attention.wv', 'layers.23.attention.wv', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w3', 'layers.23.feed_forward.w3', 'layers.24.attention.wk', 'layers.24.attention.wk', 'layers.24.attention.wo', 'layers.24.attention.wo', 'layers.24.attention.wq', 'layers.24.attention.wq', 'layers.24.attention.wv', 'layers.24.attention.wv', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w3', 'layers.24.feed_forward.w3', 'layers.25.attention.wk', 'layers.25.attention.wk', 'layers.25.attention.wo', 'layers.25.attention.wo', 'layers.25.attention.wq', 'layers.25.attention.wq', 'layers.25.attention.wv', 'layers.25.attention.wv', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w3', 'layers.25.feed_forward.w3', 'layers.26.attention.wk', 'layers.26.attention.wk', 'layers.26.attention.wo', 'layers.26.attention.wo', 'layers.26.attention.wq', 'layers.26.attention.wq', 'layers.26.attention.wv', 'layers.26.attention.wv', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w3', 'layers.26.feed_forward.w3', 'layers.27.attention.wk', 'layers.27.attention.wk', 'layers.27.attention.wo', 'layers.27.attention.wo', 'layers.27.attention.wq', 'layers.27.attention.wq', 'layers.27.attention.wv', 'layers.27.attention.wv', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w3', 'layers.27.feed_forward.w3', 'layers.28.attention.wk', 'layers.28.attention.wk', 'layers.28.attention.wo', 'layers.28.attention.wo', 'layers.28.attention.wq', 'layers.28.attention.wq', 'layers.28.attention.wv', 'layers.28.attention.wv', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w3', 'layers.28.feed_forward.w3', 'layers.29.attention.wk', 'layers.29.attention.wk', 'layers.29.attention.wo', 'layers.29.attention.wo', 'layers.29.attention.wq', 'layers.29.attention.wq', 'layers.29.attention.wv', 'layers.29.attention.wv', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w3', 'layers.29.feed_forward.w3', 'layers.3.attention.wk', 'layers.3.attention.wk', 'layers.3.attention.wo', 'layers.3.attention.wo', 'layers.3.attention.wq', 'layers.3.attention.wq', 'layers.3.attention.wv', 'layers.3.attention.wv', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w3', 'layers.3.feed_forward.w3', 'layers.30.attention.wk', 'layers.30.attention.wk', 'layers.30.attention.wo', 'layers.30.attention.wo', 'layers.30.attention.wq', 'layers.30.attention.wq', 'layers.30.attention.wv', 'layers.30.attention.wv', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w3', 'layers.30.feed_forward.w3', 'layers.31.attention.wk', 'layers.31.attention.wk', 'layers.31.attention.wo', 'layers.31.attention.wo', 'layers.31.attention.wq', 'layers.31.attention.wq', 'layers.31.attention.wv', 'layers.31.attention.wv', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w3', 'layers.31.feed_forward.w3', 'layers.4.attention.wk', 'layers.4.attention.wk', 'layers.4.attention.wo', 'layers.4.attention.wo', 'layers.4.attention.wq', 'layers.4.attention.wq', 'layers.4.attention.wv', 'layers.4.attention.wv', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w3', 'layers.4.feed_forward.w3', 'layers.5.attention.wk', 'layers.5.attention.wk', 'layers.5.attention.wo', 'layers.5.attention.wo', 'layers.5.attention.wq', 'layers.5.attention.wq', 'layers.5.attention.wv', 'layers.5.attention.wv', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w3', 'layers.5.feed_forward.w3', 'layers.6.attention.wk', 'layers.6.attention.wk', 'layers.6.attention.wo', 'layers.6.attention.wo', 'layers.6.attention.wq', 'layers.6.attention.wq', 'layers.6.attention.wv', 'layers.6.attention.wv', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w3', 'layers.6.feed_forward.w3', 'layers.7.attention.wk', 'layers.7.attention.wk', 'layers.7.attention.wo', 'layers.7.attention.wo', 'layers.7.attention.wq', 'layers.7.attention.wq', 'layers.7.attention.wv', 'layers.7.attention.wv', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w3', 'layers.7.feed_forward.w3', 'layers.8.attention.wk', 'layers.8.attention.wk', 'layers.8.attention.wo', 'layers.8.attention.wo', 'layers.8.attention.wq', 'layers.8.attention.wq', 'layers.8.attention.wv', 'layers.8.attention.wv', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w3', 'layers.8.feed_forward.w3', 'layers.9.attention.wk', 'layers.9.attention.wk', 'layers.9.attention.wo', 'layers.9.attention.wo', 'layers.9.attention.wq', 'layers.9.attention.wq', 'layers.9.attention.wv', 'layers.9.attention.wv', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w3', 'layers.9.feed_forward.w3']. Please verify that the loaded LoRA module is correct\n",
      "ERROR 06-17 17:30:29 [core.py:517] EngineCore encountered a fatal error.\n",
      "ERROR 06-17 17:30:29 [core.py:517] Traceback (most recent call last):\n",
      "ERROR 06-17 17:30:29 [core.py:517]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 508, in run_engine_core\n",
      "ERROR 06-17 17:30:29 [core.py:517]     engine_core.run_busy_loop()\n",
      "ERROR 06-17 17:30:29 [core.py:517]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 535, in run_busy_loop\n",
      "ERROR 06-17 17:30:29 [core.py:517]     self._process_engine_step()\n",
      "ERROR 06-17 17:30:29 [core.py:517]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 560, in _process_engine_step\n",
      "ERROR 06-17 17:30:29 [core.py:517]     outputs, model_executed = self.step_fn()\n",
      "ERROR 06-17 17:30:29 [core.py:517]                               ^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [core.py:517]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 231, in step\n",
      "ERROR 06-17 17:30:29 [core.py:517]     model_output = self.execute_model(scheduler_output)\n",
      "ERROR 06-17 17:30:29 [core.py:517]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [core.py:517]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 217, in execute_model\n",
      "ERROR 06-17 17:30:29 [core.py:517]     raise err\n",
      "ERROR 06-17 17:30:29 [core.py:517]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 211, in execute_model\n",
      "ERROR 06-17 17:30:29 [core.py:517]     return self.model_executor.execute_model(scheduler_output)\n",
      "ERROR 06-17 17:30:29 [core.py:517]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [core.py:517]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 163, in execute_model\n",
      "ERROR 06-17 17:30:29 [core.py:517]     (output, ) = self.collective_rpc(\"execute_model\",\n",
      "ERROR 06-17 17:30:29 [core.py:517]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [core.py:517]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 220, in collective_rpc\n",
      "ERROR 06-17 17:30:29 [core.py:517]     result = get_response(w, dequeue_timeout)\n",
      "ERROR 06-17 17:30:29 [core.py:517]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 06-17 17:30:29 [core.py:517]   File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 207, in get_response\n",
      "ERROR 06-17 17:30:29 [core.py:517]     raise RuntimeError(\n",
      "ERROR 06-17 17:30:29 [core.py:517] RuntimeError: Worker failed with error 'While loading /home/ttsai/mulkooo/sj_Trading/adapter, expected target modules in ['embed_tokens', 'lm_head', 'v_proj', 'gate_proj', 'o_proj', 'up_proj', 'k_proj', 'down_proj', 'q_proj'] but received ['layers.0.attention.wk', 'layers.0.attention.wk', 'layers.0.attention.wo', 'layers.0.attention.wo', 'layers.0.attention.wq', 'layers.0.attention.wq', 'layers.0.attention.wv', 'layers.0.attention.wv', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w3', 'layers.0.feed_forward.w3', 'layers.1.attention.wk', 'layers.1.attention.wk', 'layers.1.attention.wo', 'layers.1.attention.wo', 'layers.1.attention.wq', 'layers.1.attention.wq', 'layers.1.attention.wv', 'layers.1.attention.wv', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w3', 'layers.1.feed_forward.w3', 'layers.10.attention.wk', 'layers.10.attention.wk', 'layers.10.attention.wo', 'layers.10.attention.wo', 'layers.10.attention.wq', 'layers.10.attention.wq', 'layers.10.attention.wv', 'layers.10.attention.wv', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w3', 'layers.10.feed_forward.w3', 'layers.11.attention.wk', 'layers.11.attention.wk', 'layers.11.attention.wo', 'layers.11.attention.wo', 'layers.11.attention.wq', 'layers.11.attention.wq', 'layers.11.attention.wv', 'layers.11.attention.wv', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w3', 'layers.11.feed_forward.w3', 'layers.12.attention.wk', 'layers.12.attention.wk', 'layers.12.attention.wo', 'layers.12.attention.wo', 'layers.12.attention.wq', 'layers.12.attention.wq', 'layers.12.attention.wv', 'layers.12.attention.wv', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w3', 'layers.12.feed_forward.w3', 'layers.13.attention.wk', 'layers.13.attention.wk', 'layers.13.attention.wo', 'layers.13.attention.wo', 'layers.13.attention.wq', 'layers.13.attention.wq', 'layers.13.attention.wv', 'layers.13.attention.wv', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w3', 'layers.13.feed_forward.w3', 'layers.14.attention.wk', 'layers.14.attention.wk', 'layers.14.attention.wo', 'layers.14.attention.wo', 'layers.14.attention.wq', 'layers.14.attention.wq', 'layers.14.attention.wv', 'layers.14.attention.wv', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w3', 'layers.14.feed_forward.w3', 'layers.15.attention.wk', 'layers.15.attention.wk', 'layers.15.attention.wo', 'layers.15.attention.wo', 'layers.15.attention.wq', 'layers.15.attention.wq', 'layers.15.attention.wv', 'layers.15.attention.wv', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w3', 'layers.15.feed_forward.w3', 'layers.16.attention.wk', 'layers.16.attention.wk', 'layers.16.attention.wo', 'layers.16.attention.wo', 'layers.16.attention.wq', 'layers.16.attention.wq', 'layers.16.attention.wv', 'layers.16.attention.wv', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w3', 'layers.16.feed_forward.w3', 'layers.17.attention.wk', 'layers.17.attention.wk', 'layers.17.attention.wo', 'layers.17.attention.wo', 'layers.17.attention.wq', 'layers.17.attention.wq', 'layers.17.attention.wv', 'layers.17.attention.wv', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w3', 'layers.17.feed_forward.w3', 'layers.18.attention.wk', 'layers.18.attention.wk', 'layers.18.attention.wo', 'layers.18.attention.wo', 'layers.18.attention.wq', 'layers.18.attention.wq', 'layers.18.attention.wv', 'layers.18.attention.wv', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w3', 'layers.18.feed_forward.w3', 'layers.19.attention.wk', 'layers.19.attention.wk', 'layers.19.attention.wo', 'layers.19.attention.wo', 'layers.19.attention.wq', 'layers.19.attention.wq', 'layers.19.attention.wv', 'layers.19.attention.wv', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w3', 'layers.19.feed_forward.w3', 'layers.2.attention.wk', 'layers.2.attention.wk', 'layers.2.attention.wo', 'layers.2.attention.wo', 'layers.2.attention.wq', 'layers.2.attention.wq', 'layers.2.attention.wv', 'layers.2.attention.wv', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w3', 'layers.2.feed_forward.w3', 'layers.20.attention.wk', 'layers.20.attention.wk', 'layers.20.attention.wo', 'layers.20.attention.wo', 'layers.20.attention.wq', 'layers.20.attention.wq', 'layers.20.attention.wv', 'layers.20.attention.wv', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w3', 'layers.20.feed_forward.w3', 'layers.21.attention.wk', 'layers.21.attention.wk', 'layers.21.attention.wo', 'layers.21.attention.wo', 'layers.21.attention.wq', 'layers.21.attention.wq', 'layers.21.attention.wv', 'layers.21.attention.wv', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w3', 'layers.21.feed_forward.w3', 'layers.22.attention.wk', 'layers.22.attention.wk', 'layers.22.attention.wo', 'layers.22.attention.wo', 'layers.22.attention.wq', 'layers.22.attention.wq', 'layers.22.attention.wv', 'layers.22.attention.wv', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w3', 'layers.22.feed_forward.w3', 'layers.23.attention.wk', 'layers.23.attention.wk', 'layers.23.attention.wo', 'layers.23.attention.wo', 'layers.23.attention.wq', 'layers.23.attention.wq', 'layers.23.attention.wv', 'layers.23.attention.wv', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w3', 'layers.23.feed_forward.w3', 'layers.24.attention.wk', 'layers.24.attention.wk', 'layers.24.attention.wo', 'layers.24.attention.wo', 'layers.24.attention.wq', 'layers.24.attention.wq', 'layers.24.attention.wv', 'layers.24.attention.wv', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w3', 'layers.24.feed_forward.w3', 'layers.25.attention.wk', 'layers.25.attention.wk', 'layers.25.attention.wo', 'layers.25.attention.wo', 'layers.25.attention.wq', 'layers.25.attention.wq', 'layers.25.attention.wv', 'layers.25.attention.wv', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w3', 'layers.25.feed_forward.w3', 'layers.26.attention.wk', 'layers.26.attention.wk', 'layers.26.attention.wo', 'layers.26.attention.wo', 'layers.26.attention.wq', 'layers.26.attention.wq', 'layers.26.attention.wv', 'layers.26.attention.wv', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w3', 'layers.26.feed_forward.w3', 'layers.27.attention.wk', 'layers.27.attention.wk', 'layers.27.attention.wo', 'layers.27.attention.wo', 'layers.27.attention.wq', 'layers.27.attention.wq', 'layers.27.attention.wv', 'layers.27.attention.wv', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w3', 'layers.27.feed_forward.w3', 'layers.28.attention.wk', 'layers.28.attention.wk', 'layers.28.attention.wo', 'layers.28.attention.wo', 'layers.28.attention.wq', 'layers.28.attention.wq', 'layers.28.attention.wv', 'layers.28.attention.wv', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w3', 'layers.28.feed_forward.w3', 'layers.29.attention.wk', 'layers.29.attention.wk', 'layers.29.attention.wo', 'layers.29.attention.wo', 'layers.29.attention.wq', 'layers.29.attention.wq', 'layers.29.attention.wv', 'layers.29.attention.wv', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w3', 'layers.29.feed_forward.w3', 'layers.3.attention.wk', 'layers.3.attention.wk', 'layers.3.attention.wo', 'layers.3.attention.wo', 'layers.3.attention.wq', 'layers.3.attention.wq', 'layers.3.attention.wv', 'layers.3.attention.wv', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w3', 'layers.3.feed_forward.w3', 'layers.30.attention.wk', 'layers.30.attention.wk', 'layers.30.attention.wo', 'layers.30.attention.wo', 'layers.30.attention.wq', 'layers.30.attention.wq', 'layers.30.attention.wv', 'layers.30.attention.wv', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w3', 'layers.30.feed_forward.w3', 'layers.31.attention.wk', 'layers.31.attention.wk', 'layers.31.attention.wo', 'layers.31.attention.wo', 'layers.31.attention.wq', 'layers.31.attention.wq', 'layers.31.attention.wv', 'layers.31.attention.wv', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w3', 'layers.31.feed_forward.w3', 'layers.4.attention.wk', 'layers.4.attention.wk', 'layers.4.attention.wo', 'layers.4.attention.wo', 'layers.4.attention.wq', 'layers.4.attention.wq', 'layers.4.attention.wv', 'layers.4.attention.wv', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w3', 'layers.4.feed_forward.w3', 'layers.5.attention.wk', 'layers.5.attention.wk', 'layers.5.attention.wo', 'layers.5.attention.wo', 'layers.5.attention.wq', 'layers.5.attention.wq', 'layers.5.attention.wv', 'layers.5.attention.wv', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w3', 'layers.5.feed_forward.w3', 'layers.6.attention.wk', 'layers.6.attention.wk', 'layers.6.attention.wo', 'layers.6.attention.wo', 'layers.6.attention.wq', 'layers.6.attention.wq', 'layers.6.attention.wv', 'layers.6.attention.wv', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w3', 'layers.6.feed_forward.w3', 'layers.7.attention.wk', 'layers.7.attention.wk', 'layers.7.attention.wo', 'layers.7.attention.wo', 'layers.7.attention.wq', 'layers.7.attention.wq', 'layers.7.attention.wv', 'layers.7.attention.wv', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w3', 'layers.7.feed_forward.w3', 'layers.8.attention.wk', 'layers.8.attention.wk', 'layers.8.attention.wo', 'layers.8.attention.wo', 'layers.8.attention.wq', 'layers.8.attention.wq', 'layers.8.attention.wv', 'layers.8.attention.wv', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w3', 'layers.8.feed_forward.w3', 'layers.9.attention.wk', 'layers.9.attention.wk', 'layers.9.attention.wo', 'layers.9.attention.wo', 'layers.9.attention.wq', 'layers.9.attention.wq', 'layers.9.attention.wv', 'layers.9.attention.wv', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w3', 'layers.9.feed_forward.w3']. Please verify that the loaded LoRA module is correct', please check the stack trace above for the root cause\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=274722)\u001b[0;0m ERROR 06-17 17:30:29 [multiproc_executor.py:527] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ttsai/miniconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ttsai/miniconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 519, in run_engine_core\n",
      "    raise e\n",
      "  File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 508, in run_engine_core\n",
      "    engine_core.run_busy_loop()\n",
      "  File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 535, in run_busy_loop\n",
      "    self._process_engine_step()\n",
      "  File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 560, in _process_engine_step\n",
      "    outputs, model_executed = self.step_fn()\n",
      "                              ^^^^^^^^^^^^^^\n",
      "  File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 231, in step\n",
      "    model_output = self.execute_model(scheduler_output)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 217, in execute_model\n",
      "    raise err\n",
      "  File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core.py\", line 211, in execute_model\n",
      "    return self.model_executor.execute_model(scheduler_output)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 163, in execute_model\n",
      "    (output, ) = self.collective_rpc(\"execute_model\",\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 220, in collective_rpc\n",
      "    result = get_response(w, dequeue_timeout)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py\", line 207, in get_response\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Worker failed with error 'While loading /home/ttsai/mulkooo/sj_Trading/adapter, expected target modules in ['embed_tokens', 'lm_head', 'v_proj', 'gate_proj', 'o_proj', 'up_proj', 'k_proj', 'down_proj', 'q_proj'] but received ['layers.0.attention.wk', 'layers.0.attention.wk', 'layers.0.attention.wo', 'layers.0.attention.wo', 'layers.0.attention.wq', 'layers.0.attention.wq', 'layers.0.attention.wv', 'layers.0.attention.wv', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w1', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w2', 'layers.0.feed_forward.w3', 'layers.0.feed_forward.w3', 'layers.1.attention.wk', 'layers.1.attention.wk', 'layers.1.attention.wo', 'layers.1.attention.wo', 'layers.1.attention.wq', 'layers.1.attention.wq', 'layers.1.attention.wv', 'layers.1.attention.wv', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w1', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w2', 'layers.1.feed_forward.w3', 'layers.1.feed_forward.w3', 'layers.10.attention.wk', 'layers.10.attention.wk', 'layers.10.attention.wo', 'layers.10.attention.wo', 'layers.10.attention.wq', 'layers.10.attention.wq', 'layers.10.attention.wv', 'layers.10.attention.wv', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w1', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w2', 'layers.10.feed_forward.w3', 'layers.10.feed_forward.w3', 'layers.11.attention.wk', 'layers.11.attention.wk', 'layers.11.attention.wo', 'layers.11.attention.wo', 'layers.11.attention.wq', 'layers.11.attention.wq', 'layers.11.attention.wv', 'layers.11.attention.wv', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w1', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w2', 'layers.11.feed_forward.w3', 'layers.11.feed_forward.w3', 'layers.12.attention.wk', 'layers.12.attention.wk', 'layers.12.attention.wo', 'layers.12.attention.wo', 'layers.12.attention.wq', 'layers.12.attention.wq', 'layers.12.attention.wv', 'layers.12.attention.wv', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w1', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w2', 'layers.12.feed_forward.w3', 'layers.12.feed_forward.w3', 'layers.13.attention.wk', 'layers.13.attention.wk', 'layers.13.attention.wo', 'layers.13.attention.wo', 'layers.13.attention.wq', 'layers.13.attention.wq', 'layers.13.attention.wv', 'layers.13.attention.wv', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w1', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w2', 'layers.13.feed_forward.w3', 'layers.13.feed_forward.w3', 'layers.14.attention.wk', 'layers.14.attention.wk', 'layers.14.attention.wo', 'layers.14.attention.wo', 'layers.14.attention.wq', 'layers.14.attention.wq', 'layers.14.attention.wv', 'layers.14.attention.wv', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w1', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w2', 'layers.14.feed_forward.w3', 'layers.14.feed_forward.w3', 'layers.15.attention.wk', 'layers.15.attention.wk', 'layers.15.attention.wo', 'layers.15.attention.wo', 'layers.15.attention.wq', 'layers.15.attention.wq', 'layers.15.attention.wv', 'layers.15.attention.wv', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w1', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w2', 'layers.15.feed_forward.w3', 'layers.15.feed_forward.w3', 'layers.16.attention.wk', 'layers.16.attention.wk', 'layers.16.attention.wo', 'layers.16.attention.wo', 'layers.16.attention.wq', 'layers.16.attention.wq', 'layers.16.attention.wv', 'layers.16.attention.wv', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w1', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w2', 'layers.16.feed_forward.w3', 'layers.16.feed_forward.w3', 'layers.17.attention.wk', 'layers.17.attention.wk', 'layers.17.attention.wo', 'layers.17.attention.wo', 'layers.17.attention.wq', 'layers.17.attention.wq', 'layers.17.attention.wv', 'layers.17.attention.wv', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w1', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w2', 'layers.17.feed_forward.w3', 'layers.17.feed_forward.w3', 'layers.18.attention.wk', 'layers.18.attention.wk', 'layers.18.attention.wo', 'layers.18.attention.wo', 'layers.18.attention.wq', 'layers.18.attention.wq', 'layers.18.attention.wv', 'layers.18.attention.wv', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w1', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w2', 'layers.18.feed_forward.w3', 'layers.18.feed_forward.w3', 'layers.19.attention.wk', 'layers.19.attention.wk', 'layers.19.attention.wo', 'layers.19.attention.wo', 'layers.19.attention.wq', 'layers.19.attention.wq', 'layers.19.attention.wv', 'layers.19.attention.wv', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w1', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w2', 'layers.19.feed_forward.w3', 'layers.19.feed_forward.w3', 'layers.2.attention.wk', 'layers.2.attention.wk', 'layers.2.attention.wo', 'layers.2.attention.wo', 'layers.2.attention.wq', 'layers.2.attention.wq', 'layers.2.attention.wv', 'layers.2.attention.wv', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w1', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w2', 'layers.2.feed_forward.w3', 'layers.2.feed_forward.w3', 'layers.20.attention.wk', 'layers.20.attention.wk', 'layers.20.attention.wo', 'layers.20.attention.wo', 'layers.20.attention.wq', 'layers.20.attention.wq', 'layers.20.attention.wv', 'layers.20.attention.wv', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w1', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w2', 'layers.20.feed_forward.w3', 'layers.20.feed_forward.w3', 'layers.21.attention.wk', 'layers.21.attention.wk', 'layers.21.attention.wo', 'layers.21.attention.wo', 'layers.21.attention.wq', 'layers.21.attention.wq', 'layers.21.attention.wv', 'layers.21.attention.wv', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w1', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w2', 'layers.21.feed_forward.w3', 'layers.21.feed_forward.w3', 'layers.22.attention.wk', 'layers.22.attention.wk', 'layers.22.attention.wo', 'layers.22.attention.wo', 'layers.22.attention.wq', 'layers.22.attention.wq', 'layers.22.attention.wv', 'layers.22.attention.wv', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w1', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w2', 'layers.22.feed_forward.w3', 'layers.22.feed_forward.w3', 'layers.23.attention.wk', 'layers.23.attention.wk', 'layers.23.attention.wo', 'layers.23.attention.wo', 'layers.23.attention.wq', 'layers.23.attention.wq', 'layers.23.attention.wv', 'layers.23.attention.wv', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w1', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w2', 'layers.23.feed_forward.w3', 'layers.23.feed_forward.w3', 'layers.24.attention.wk', 'layers.24.attention.wk', 'layers.24.attention.wo', 'layers.24.attention.wo', 'layers.24.attention.wq', 'layers.24.attention.wq', 'layers.24.attention.wv', 'layers.24.attention.wv', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w1', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w2', 'layers.24.feed_forward.w3', 'layers.24.feed_forward.w3', 'layers.25.attention.wk', 'layers.25.attention.wk', 'layers.25.attention.wo', 'layers.25.attention.wo', 'layers.25.attention.wq', 'layers.25.attention.wq', 'layers.25.attention.wv', 'layers.25.attention.wv', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w1', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w2', 'layers.25.feed_forward.w3', 'layers.25.feed_forward.w3', 'layers.26.attention.wk', 'layers.26.attention.wk', 'layers.26.attention.wo', 'layers.26.attention.wo', 'layers.26.attention.wq', 'layers.26.attention.wq', 'layers.26.attention.wv', 'layers.26.attention.wv', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w1', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w2', 'layers.26.feed_forward.w3', 'layers.26.feed_forward.w3', 'layers.27.attention.wk', 'layers.27.attention.wk', 'layers.27.attention.wo', 'layers.27.attention.wo', 'layers.27.attention.wq', 'layers.27.attention.wq', 'layers.27.attention.wv', 'layers.27.attention.wv', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w1', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w2', 'layers.27.feed_forward.w3', 'layers.27.feed_forward.w3', 'layers.28.attention.wk', 'layers.28.attention.wk', 'layers.28.attention.wo', 'layers.28.attention.wo', 'layers.28.attention.wq', 'layers.28.attention.wq', 'layers.28.attention.wv', 'layers.28.attention.wv', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w1', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w2', 'layers.28.feed_forward.w3', 'layers.28.feed_forward.w3', 'layers.29.attention.wk', 'layers.29.attention.wk', 'layers.29.attention.wo', 'layers.29.attention.wo', 'layers.29.attention.wq', 'layers.29.attention.wq', 'layers.29.attention.wv', 'layers.29.attention.wv', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w1', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w2', 'layers.29.feed_forward.w3', 'layers.29.feed_forward.w3', 'layers.3.attention.wk', 'layers.3.attention.wk', 'layers.3.attention.wo', 'layers.3.attention.wo', 'layers.3.attention.wq', 'layers.3.attention.wq', 'layers.3.attention.wv', 'layers.3.attention.wv', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w1', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w2', 'layers.3.feed_forward.w3', 'layers.3.feed_forward.w3', 'layers.30.attention.wk', 'layers.30.attention.wk', 'layers.30.attention.wo', 'layers.30.attention.wo', 'layers.30.attention.wq', 'layers.30.attention.wq', 'layers.30.attention.wv', 'layers.30.attention.wv', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w1', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w2', 'layers.30.feed_forward.w3', 'layers.30.feed_forward.w3', 'layers.31.attention.wk', 'layers.31.attention.wk', 'layers.31.attention.wo', 'layers.31.attention.wo', 'layers.31.attention.wq', 'layers.31.attention.wq', 'layers.31.attention.wv', 'layers.31.attention.wv', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w1', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w2', 'layers.31.feed_forward.w3', 'layers.31.feed_forward.w3', 'layers.4.attention.wk', 'layers.4.attention.wk', 'layers.4.attention.wo', 'layers.4.attention.wo', 'layers.4.attention.wq', 'layers.4.attention.wq', 'layers.4.attention.wv', 'layers.4.attention.wv', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w1', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w2', 'layers.4.feed_forward.w3', 'layers.4.feed_forward.w3', 'layers.5.attention.wk', 'layers.5.attention.wk', 'layers.5.attention.wo', 'layers.5.attention.wo', 'layers.5.attention.wq', 'layers.5.attention.wq', 'layers.5.attention.wv', 'layers.5.attention.wv', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w1', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w2', 'layers.5.feed_forward.w3', 'layers.5.feed_forward.w3', 'layers.6.attention.wk', 'layers.6.attention.wk', 'layers.6.attention.wo', 'layers.6.attention.wo', 'layers.6.attention.wq', 'layers.6.attention.wq', 'layers.6.attention.wv', 'layers.6.attention.wv', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w1', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w2', 'layers.6.feed_forward.w3', 'layers.6.feed_forward.w3', 'layers.7.attention.wk', 'layers.7.attention.wk', 'layers.7.attention.wo', 'layers.7.attention.wo', 'layers.7.attention.wq', 'layers.7.attention.wq', 'layers.7.attention.wv', 'layers.7.attention.wv', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w1', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w2', 'layers.7.feed_forward.w3', 'layers.7.feed_forward.w3', 'layers.8.attention.wk', 'layers.8.attention.wk', 'layers.8.attention.wo', 'layers.8.attention.wo', 'layers.8.attention.wq', 'layers.8.attention.wq', 'layers.8.attention.wv', 'layers.8.attention.wv', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w1', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w2', 'layers.8.feed_forward.w3', 'layers.8.feed_forward.w3', 'layers.9.attention.wk', 'layers.9.attention.wk', 'layers.9.attention.wo', 'layers.9.attention.wo', 'layers.9.attention.wq', 'layers.9.attention.wq', 'layers.9.attention.wv', 'layers.9.attention.wv', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w1', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w2', 'layers.9.feed_forward.w3', 'layers.9.feed_forward.w3']. Please verify that the loaded LoRA module is correct', please check the stack trace above for the root cause\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    enable_lora=True,\n",
    "    tokenizer_mode=\"mistral\",\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.90,\n",
    "    max_model_len=2048,\n",
    "    max_num_seqs=64,\n",
    "    tensor_parallel_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaa39120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "# Create a LoRA request with the PEFT adapter\n",
    "lora_request = LoRARequest(\n",
    "    \"financial_adapter\", # Just human readable\n",
    "    1,\n",
    "    lora_path=\"/home/ttsai/mulkooo/sj_Trading/adapter\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b0df475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f274eae25b420cb9d6a9a119555f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85abd14f7f8e460d8cd5cc521bbc5bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "EngineDeadError",
     "evalue": "EngineCore encountered an issue. See stack trace (above) for the root cause.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEngineDeadError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Genearate with the LoRA adapter\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m outputs = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWrite a financial analysis of Tesla.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Print the generated text\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/utils.py:1267\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1260\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1262\u001b[39m         warnings.warn(\n\u001b[32m   1263\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1264\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1265\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:474\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[39m\n\u001b[32m    462\u001b[39m     sampling_params = \u001b[38;5;28mself\u001b[39m.get_default_sampling_params()\n\u001b[32m    464\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    465\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    466\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    471\u001b[39m     priority=priority,\n\u001b[32m    472\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:1517\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1515\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1516\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1517\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1518\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1519\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:232\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[32m    235\u001b[39m iteration_stats = IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:625\u001b[39m, in \u001b[36mSyncMPClient.get_output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    623\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.outputs_queue.get()\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[31mEngineDeadError\u001b[39m: EngineCore encountered an issue. See stack trace (above) for the root cause."
     ]
    }
   ],
   "source": [
    "# Genearate with the LoRA adapter\n",
    "outputs = llm.generate(\n",
    "    [\"Write a financial analysis of Tesla.\"],\n",
    "    sampling_params,\n",
    "    lora_request=lora_request\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ccfb1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf737390151459da80851407d251f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttsai/mulkooo/sj_Trading/.venv/lib/python3.11/site-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"/home/ttsai/mulkooo/sj_Trading/adapter\")\n",
    "\n",
    "# Merge LoRA weights into base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_model.save_pretrained(\"/home/ttsai/mulkooo/sj_Trading/merge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cc927c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 17:58:31 [config.py:823] This model supports multiple tasks: {'generate', 'reward', 'score', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 06-17 17:58:31 [config.py:3268] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 06-17 17:58:31 [config.py:1946] Defaulting to use mp for distributed inference\n",
      "INFO 06-17 17:58:31 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 06-17 17:58:32 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 06-17 17:58:33 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 06-17 17:58:35 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 06-17 17:58:35 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/home/ttsai/mulkooo/sj_Trading/merge', speculative_config=None, tokenizer='/home/ttsai/mulkooo/sj_Trading/merge', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/ttsai/mulkooo/sj_Trading/merge, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 06-17 17:58:35 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-17 17:58:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_f1e96a9e'), local_subscribe_addr='ipc:///tmp/20db05ec-3acf-4726-ba06-787304cae329', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-17 17:58:35 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "WARNING 06-17 17:58:35 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234\n",
      "INFO 06-17 17:58:36 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 06-17 17:58:36 [__init__.py:244] Automatically detected platform cuda.\n",
      "WARNING 06-17 17:58:38 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7c45df50c890>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_81e84cd2'), local_subscribe_addr='ipc:///tmp/2dff4d1e-da0c-4b18-839a-e0af670582f7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-17 17:58:38 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7a8d2d563fd0>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_949f689a'), local_subscribe_addr='ipc:///tmp/8538b156-a537-413a-9eeb-9cbcdce1764f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:38 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:38 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:38 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:38 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ttsai/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:38 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ttsai/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m WARNING 06-17 17:58:38 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m WARNING 06-17 17:58:38 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_28ce24ed'), local_subscribe_addr='ipc:///tmp/114ec8b2-3646-4bb9-9944-96e64f0ba44e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:39 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:39 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m WARNING 06-17 17:58:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m WARNING 06-17 17:58:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:39 [gpu_model_runner.py:1595] Starting to load model /home/ttsai/mulkooo/sj_Trading/merge...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:39 [gpu_model_runner.py:1595] Starting to load model /home/ttsai/mulkooo/sj_Trading/merge...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:39 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:39 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:39 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:39 [cuda.py:252] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:02,  1.75it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:01<00:02,  1.72it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:01<00:01,  1.72it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:02<00:01,  1.80it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:02<00:00,  1.74it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:03<00:00,  1.72it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:03<00:00,  1.74it/s]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:42 [default_loader.py:272] Loading weights took 3.49 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:42 [gpu_model_runner.py:1624] Model loading took 6.7584 GiB and 3.569238 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:43 [default_loader.py:272] Loading weights took 3.92 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:43 [gpu_model_runner.py:1624] Model loading took 6.7584 GiB and 4.000864 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:46 [backends.py:462] Using cache directory: /home/ttsai/.cache/vllm/torch_compile_cache/34a6c50b67/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:46 [backends.py:472] Dynamo bytecode transform time: 3.39 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:46 [backends.py:462] Using cache directory: /home/ttsai/.cache/vllm/torch_compile_cache/34a6c50b67/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:46 [backends.py:472] Dynamo bytecode transform time: 3.47 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:49 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 2.641 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:49 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 2.679 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:50 [monitor.py:34] torch.compile takes 3.39 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:50 [monitor.py:34] torch.compile takes 3.47 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:58:52 [gpu_worker.py:227] Available KV cache memory: 6.58 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:58:52 [gpu_worker.py:227] Available KV cache memory: 6.58 GiB\n",
      "INFO 06-17 17:58:53 [kv_cache_utils.py:715] GPU KV cache size: 107,872 tokens\n",
      "INFO 06-17 17:58:53 [kv_cache_utils.py:719] Maximum concurrency for 2,048 tokens per request: 52.67x\n",
      "INFO 06-17 17:58:53 [kv_cache_utils.py:715] GPU KV cache size: 107,872 tokens\n",
      "INFO 06-17 17:58:53 [kv_cache_utils.py:719] Maximum concurrency for 2,048 tokens per request: 52.67x\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=284014)\u001b[0;0m INFO 06-17 17:59:08 [gpu_model_runner.py:2048] Graph capturing finished in 16 secs, took 0.71 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=284015)\u001b[0;0m INFO 06-17 17:59:08 [gpu_model_runner.py:2048] Graph capturing finished in 16 secs, took 0.71 GiB\n",
      "INFO 06-17 17:59:08 [core.py:171] init engine (profile, create kv cache, warmup model) took 25.57 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/home/ttsai/mulkooo/sj_Trading/merge\",\n",
    "    tokenizer_mode=\"mistral\",\n",
    "    dtype=\"bfloat16\",\n",
    "    gpu_memory_utilization=0.90,\n",
    "    max_model_len=2048,\n",
    "    max_num_seqs=64,\n",
    "    tensor_parallel_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d53103fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe53555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8352e3978f4432a32d0c7b739e1b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5b2b4c25e1487180aeef7de9330c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      "Tesla, Inc. (TSLA) is an American electric vehicle (EV) and clean energy company that has disrupted the traditional automotive industry with its innovative technology and commitment to sustainability. As of 2021, Tesla is the world's most valuable automaker by market capitalization, and its stock has been notorious for its volatility.\n",
      "\n",
      "Financial Analysis:\n",
      "\n",
      "1. Revenue: In 2020, Tesla reported total revenues of $31.5 billion, an increase of 36% compared to 2019. The growth was driven by a 48% increase in vehicle deliveries to 509,738 units, as well as a 45% increase in regulatory credits to $1.2 billion.\n",
      "\n",
      "2. Gross Margin: Tesla's gross margin improved significantly in 2020, reaching 27.3%, compared to 21.7% in 2019. The improvement was due to a higher mix of Model 3 and Model Y sales, which have higher margins, as well as cost-cutting measures and economies of scale.\n",
      "\n",
      "3. Operating Expenses: Total operating expenses increased by 24% in 2020, primarily due to higher research and development (R&D) expenses, selling, general, and administrative (SG&A) expenses, and depreciation and amortization expenses.\n",
      "\n",
      "4. Net Income: Tesla reported a net income of $721 million in 2020, compared to a net loss of $862 million in 2019. The improvement was driven by higher revenues, gross margin expansion, and lower restructuring charges.\n",
      "\n",
      "5. Cash Flow: Tesla generated $2.9 billion in operating cash flow in 2020, compared to $1.1 billion in 2019. The improvement was due to higher cash flows from operating activities, partially offset by increased capital expenditures.\n",
      "\n",
      "6. Cash Position: As of December 31, 2020, Tesla had $19.4 billion in cash and cash equivalents, compared to $6.3 billion at the end of 2019. The increase was due to strong cash flow generation and the issuance of debt and equity.\n",
      "\n",
      "7. Debt: Tesla's long-term debt increased significantly in 2020, from $10.4 billion at the end of 2019 to $19.2 billion at the end of 2020. The increase was due to the issuance of bonds and convertible notes to fund the company's growth initiatives.\n",
      "\n",
      "8. Valuation: As of 2021, Tesla has a market capitalization of over $700 billion, making it the most valuable automaker in the world. The company's high valuation is based on its potential for future growth, innovative technology, and leadership position in the EV market.\n",
      "\n",
      "Investment Analysis:\n",
      "\n",
      "1. Growth Opportunities: Tesla is well-positioned to benefit from the growing EV market, which is expected to continue expanding as governments around the world implement stricter emissions regulations and provide incentives for EV adoption.\n",
      "\n",
      "2. Competitive Advantage: Tesla has a significant competitive advantage due to its vertically integrated business model, advanced battery technology, and strong brand loyalty.\n",
      "\n",
      "3. Risks: Tesla faces several risks, including intense competition from traditional automakers, regulatory risks, and supply chain disruptions. Additionally, the company's high valuation and reliance on debt make it vulnerable to market volatility and changes in interest rates.\n",
      "\n",
      "4. Recommendation: Tesla is a high-risk, high-reward investment opportunity. Long-term investors who are willing to tolerate volatility may find value in the company's potential for future growth and leadership position in the EV market. However, investors should be cautious and consider diversifying their portfolio to mitigate risk.\n",
      "\n",
      "In conclusion, Tesla has achieved significant growth and profitability in recent years, driven by its innovative technology, commitment to sustainability, and leadership position in the EV market. While the company faces several risks, its growth opportunities and competitive advantage make it an attractive investment for those who are willing to tolerate volatility. Long-term investors may find value in Tesla's potential for future growth, but should be cautious and consider diversifying their portfolio to mitigate risk.\n"
     ]
    }
   ],
   "source": [
    "# Genearate with the LoRA adapter\n",
    "outputs = llm.generate(\n",
    "    [\"Write a financial analysis of Tesla.\"],\n",
    "    sampling_params\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cd5fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# If using distributed parallelism, import destroy functions\n",
    "from vllm.distributed.parallel_state import (\n",
    "    destroy_model_parallel, destroy_distributed_environment\n",
    ")\n",
    "\n",
    "# Delete model parallel/distributed environments\n",
    "destroy_model_parallel()\n",
    "destroy_distributed_environment()\n",
    "\n",
    "# Delete the LLM object\n",
    "del llm\n",
    "\n",
    "# Clean up Python and GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
