{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81835749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sj_Trading/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.llms import VLLM\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv('hf_token')\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9dafc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 16:29:10 [config.py:793] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 06-13 16:29:10 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sj_Trading/.venv/lib/python3.11/site-packages/vllm/transformers_utils/tokenizer_group.py:23: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode \"mistral\"` to ensure correct encoding and decoding.\n",
      "  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 16:29:17 [core.py:438] Waiting for init message from front-end.\n",
      "INFO 06-13 16:29:17 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='mistralai/Mistral-7B-Instruct-v0.3', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "WARNING 06-13 16:29:18 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fbe16f50910>\n",
      "INFO 06-13 16:29:19 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-13 16:29:19 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-13 16:29:19 [gpu_model_runner.py:1531] Starting to load model mistralai/Mistral-7B-Instruct-v0.3...\n",
      "INFO 06-13 16:29:19 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-13 16:29:19 [backends.py:35] Using InductorAdaptor\n",
      "INFO 06-13 16:29:20 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "INFO 06-13 16:33:33 [weight_utils.py:307] Time spent downloading weights for mistralai/Mistral-7B-Instruct-v0.3: 253.116752 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.26it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 16:33:36 [default_loader.py:280] Loading weights took 2.68 seconds\n",
      "INFO 06-13 16:33:37 [gpu_model_runner.py:1549] Model loading took 13.5084 GiB and 257.676172 seconds\n",
      "INFO 06-13 16:33:43 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/0f747d5ef9/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-13 16:33:43 [backends.py:469] Dynamo bytecode transform time: 6.11 s\n",
      "INFO 06-13 16:33:46 [backends.py:158] Cache the graph of shape None for later use\n",
      "INFO 06-13 16:34:06 [backends.py:170] Compiling a graph for general shape takes 21.54 s\n",
      "INFO 06-13 16:34:14 [monitor.py:33] torch.compile takes 27.66 s in total\n",
      "INFO 06-13 16:34:15 [kv_cache_utils.py:637] GPU KV cache size: 229,040 tokens\n",
      "INFO 06-13 16:34:15 [kv_cache_utils.py:640] Maximum concurrency for 32,768 tokens per request: 6.99x\n",
      "INFO 06-13 16:34:44 [gpu_model_runner.py:1933] Graph capturing finished in 29 secs, took 0.51 GiB\n",
      "INFO 06-13 16:34:44 [core.py:167] init engine (profile, create kv cache, warmup model) took 67.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sj_Trading/.venv/lib/python3.11/site-packages/vllm/transformers_utils/tokenizer_group.py:23: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode \"mistral\"` to ensure correct encoding and decoding.\n",
      "  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 231.86it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.31s/it, est. speed input: 3.47 toks/s, output: 44.19 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Paris is the capital city of France. It is located in the north-central part of the country on the river Seine. Paris is the largest city in France and is known for its famous landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. The city has a rich history and is one of the most popular tourist destinations in the world. It is also an important cultural, economic, and political center of France.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = VLLM(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    token=hf_token,\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=128,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9b88dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up vllm process\n",
    "# 1. Delete the LLM object\n",
    "del llm\n",
    "\n",
    "# 2. Clear PyTorch's GPU cache\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 3. Force garbage collection\n",
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
